{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3807ce0-84a5-4a86-8387-c20e3431801d",
   "metadata": {},
   "source": [
    "# Image analyzer pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04cbb2da-fec1-47f5-a71d-b6e6d8a97f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mauri\\Desktop\\Facultad\\Tesis\\Pasantia2\\mastcell-data-main\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from skimage.io import imread\n",
    "from skimage import measure\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import random\n",
    "from Unet_MCT import Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acb00f08-9bb0-4334-a2f7-cbd94597aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# support methods\n",
    "from skimage import morphology\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def getDividableNumber(n, divisor=64): # Esta funcion da la division entera, si hay resto da uno mas\n",
    "    remainer = n % divisor\n",
    "    if remainer == 0:\n",
    "        return n\n",
    "\n",
    "    return int(n + (divisor - remainer))\n",
    "\n",
    "def zeroPad(img, size): # Le crea un borde a la imagen\n",
    "\n",
    "    old_size = img.shape[:2]\n",
    "    delta_w = abs(old_size[1] - size)\n",
    "    delta_h = abs(old_size[0] - size)\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "    \n",
    "    color = [0, 0, 0]\n",
    "    new_img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_REFLECT,value=color) # Crea un borde\n",
    "    \n",
    "    return new_img,[top, bottom, left, right]\n",
    "\n",
    "\n",
    "def filterBySize(mask,mycMinDia,factor): # Da una mascara binaria con todos los objetos mayores a cierta area\n",
    "\n",
    "    binaryImg=mask.copy()\n",
    "    binaryImg[binaryImg>0]=1\n",
    "    pxMinSize =(np.power((mycMinDia/factor/2),2)*np.pi) #~7ym² -> pixel area of a circle with the set diameter\n",
    "    binaryImg = morphology.remove_small_objects(binaryImg.astype('bool'), pxMinSize, connectivity=8)\n",
    "    copyMask=mask.copy()\n",
    "    copyMask[binaryImg==0]=0\n",
    "    return copyMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f2ba78f-96df-4141-9a21-d0be944b1a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resShape has to be set depending on the image size -> default could lead to an unnecessary high amount of memory usage\n",
    "def applyModeltoGetIdMask(model,img,resShape=-1,mycMinDia=3,factor=0.248,do_filter=False):\n",
    "    if (resShape==-1):\n",
    "        resShape=getDividableNumber(int(np.max(np.shape(img))))\n",
    "    #prepare image\n",
    "    imgResh,[top, bottom, left, right]=zeroPad(img.copy(),resShape)\n",
    "    \n",
    "    \n",
    "    #apply model\n",
    "    t_img=torch.tensor(imgResh.transpose(2,0,1)).unsqueeze(dim=0)\n",
    "    t_img=t_img.to(device)\n",
    "    seg=model(t_img).detach().requires_grad_(False).to('cpu').squeeze().numpy()\n",
    "    seg=seg[top:resShape-bottom, left:resShape-right]\n",
    "    seg[seg>=0.5]=1\n",
    "    seg[seg<0.5]=0\n",
    "    \n",
    "    if(do_filter):\n",
    "        seg=filterBySize(seg,mycMinDia,factor)\n",
    "    \n",
    "    labeled_seg, count_result_seg = measure.label(seg,connectivity=2, return_num=True)\n",
    "    \n",
    "    return labeled_seg\n",
    "\n",
    "def getTopN(narray,perc=0.1):\n",
    "    return np.sort(narray)[-int(len(narray)*perc):]\n",
    "\n",
    "def equArea(diameter):\n",
    "    return np.power((diameter/2.0),2)*np.pi\n",
    "\n",
    "#minDia, factor and areaFactor has to be set depending on the imaging settings and image data\n",
    "def measureImg(segResult,minDia=3,factor=0.248, areaFactor=0.061828822,trainNineQuantile=0,trainMedian=0,sol_highQuantiles=[],sol_lowQuantiles=[]):\n",
    "    labeled_result, count_result = measure.label(segResult,connectivity=2, return_num=True)\n",
    "    filtered_img=filterBySize(labeled_result,mycMinDia=minDia,factor=factor)\n",
    "    filteredIds=list(np.unique(filtered_img))\n",
    "    m=measure.regionprops(filtered_img)\n",
    "    m=[[x.eccentricity,x.solidity,x.area*areaFactor,equArea(x.equivalent_diameter*factor)] for x in m]\n",
    "    \n",
    "    areas=[x[2] for x in m]\n",
    "    \n",
    "    #top n areas\n",
    "    top_ten_area=getTopN(areas)\n",
    "    \n",
    "    ninety_quantile=np.quantile(areas, 0.90)\n",
    "    \n",
    "    #percentage above norm\n",
    "    normQuantileTest=areas>trainNineQuantile\n",
    "    normPercQuantile=(sum(normQuantileTest)/len(normQuantileTest))*100\n",
    "    \n",
    "    normMedianTest=areas>trainMedian\n",
    "    normPercMedian=(sum(normMedianTest)/len(normMedianTest))*100\n",
    "    \n",
    "    #solidity\n",
    "    sols=[x[1] for x in m]\n",
    "    sol_measures=[]\n",
    "    for sol_quant in sol_highQuantiles:\n",
    "        solQuantileTest=sols>sol_quant\n",
    "        sol_measures.append((sum(solQuantileTest)/len(solQuantileTest))*100)\n",
    "    \n",
    "    for sol_quant in sol_highQuantiles:\n",
    "        solQuantileTest=sols<sol_quant\n",
    "        sol_measures.append((sum(solQuantileTest)/len(solQuantileTest))*100)\n",
    "    sol_measures.extend(sol_highQuantiles)\n",
    "    \n",
    "    for sol_quant in sol_lowQuantiles:\n",
    "        solQuantileTest=sols<sol_quant\n",
    "        sol_measures.append((sum(solQuantileTest)/len(solQuantileTest))*100)\n",
    "    \n",
    "    sol_measures.extend(sol_lowQuantiles)\n",
    "    \n",
    "    return np.concatenate([np.concatenate([np.concatenate([np.std(m,axis=0),np.mean(m,axis=0),np.median(m,axis=0),stats.skew(m,axis=0)]), [np.median(top_ten_area),np.mean(top_ten_area),ninety_quantile/np.median(areas),ninety_quantile/np.median([np.log(x) for x in areas]),normPercQuantile,normPercMedian,ninety_quantile,np.median([np.log(x) for x in areas]),np.median(areas),trainNineQuantile,trainMedian]]),np.array(sol_measures)]),areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37948a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_device(verbose = True):\n",
    "    is_cuda = torch.cuda.is_available()\n",
    "\n",
    "    if is_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "        if verbose:\n",
    "            print(\"GPU disponible\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        if verbose:\n",
    "            print(\"GPU no disponible, usando CPU\")\n",
    "    \n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55faa8ea-a0cc-4a9c-91fc-eb662012df2c",
   "metadata": {},
   "source": [
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c2be61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model.encoder.model.stem.conv.weight', 'model.encoder.model.stem.bn.weight', 'model.encoder.model.stem.bn.bias', 'model.encoder.model.stem.bn.running_mean', 'model.encoder.model.stem.bn.running_var', 'model.encoder.model.stem.bn.num_batches_tracked', 'model.encoder.model.s1.b1.conv1.conv.weight', 'model.encoder.model.s1.b1.conv1.bn.weight', 'model.encoder.model.s1.b1.conv1.bn.bias', 'model.encoder.model.s1.b1.conv1.bn.running_mean', 'model.encoder.model.s1.b1.conv1.bn.running_var', 'model.encoder.model.s1.b1.conv1.bn.num_batches_tracked', 'model.encoder.model.s1.b1.conv2.conv.weight', 'model.encoder.model.s1.b1.conv2.bn.weight', 'model.encoder.model.s1.b1.conv2.bn.bias', 'model.encoder.model.s1.b1.conv2.bn.running_mean', 'model.encoder.model.s1.b1.conv2.bn.running_var', 'model.encoder.model.s1.b1.conv2.bn.num_batches_tracked', 'model.encoder.model.s1.b1.se.fc1.weight', 'model.encoder.model.s1.b1.se.fc1.bias', 'model.encoder.model.s1.b1.se.fc2.weight', 'model.encoder.model.s1.b1.se.fc2.bias', 'model.encoder.model.s1.b1.conv3.conv.weight', 'model.encoder.model.s1.b1.conv3.bn.weight', 'model.encoder.model.s1.b1.conv3.bn.bias', 'model.encoder.model.s1.b1.conv3.bn.running_mean', 'model.encoder.model.s1.b1.conv3.bn.running_var', 'model.encoder.model.s1.b1.conv3.bn.num_batches_tracked', 'model.encoder.model.s1.b1.downsample.conv.weight', 'model.encoder.model.s1.b1.downsample.bn.weight', 'model.encoder.model.s1.b1.downsample.bn.bias', 'model.encoder.model.s1.b1.downsample.bn.running_mean', 'model.encoder.model.s1.b1.downsample.bn.running_var', 'model.encoder.model.s1.b1.downsample.bn.num_batches_tracked', 'model.encoder.model.s1.b2.conv1.conv.weight', 'model.encoder.model.s1.b2.conv1.bn.weight', 'model.encoder.model.s1.b2.conv1.bn.bias', 'model.encoder.model.s1.b2.conv1.bn.running_mean', 'model.encoder.model.s1.b2.conv1.bn.running_var', 'model.encoder.model.s1.b2.conv1.bn.num_batches_tracked', 'model.encoder.model.s1.b2.conv2.conv.weight', 'model.encoder.model.s1.b2.conv2.bn.weight', 'model.encoder.model.s1.b2.conv2.bn.bias', 'model.encoder.model.s1.b2.conv2.bn.running_mean', 'model.encoder.model.s1.b2.conv2.bn.running_var', 'model.encoder.model.s1.b2.conv2.bn.num_batches_tracked', 'model.encoder.model.s1.b2.se.fc1.weight', 'model.encoder.model.s1.b2.se.fc1.bias', 'model.encoder.model.s1.b2.se.fc2.weight', 'model.encoder.model.s1.b2.se.fc2.bias', 'model.encoder.model.s1.b2.conv3.conv.weight', 'model.encoder.model.s1.b2.conv3.bn.weight', 'model.encoder.model.s1.b2.conv3.bn.bias', 'model.encoder.model.s1.b2.conv3.bn.running_mean', 'model.encoder.model.s1.b2.conv3.bn.running_var', 'model.encoder.model.s1.b2.conv3.bn.num_batches_tracked', 'model.encoder.model.s2.b1.conv1.conv.weight', 'model.encoder.model.s2.b1.conv1.bn.weight', 'model.encoder.model.s2.b1.conv1.bn.bias', 'model.encoder.model.s2.b1.conv1.bn.running_mean', 'model.encoder.model.s2.b1.conv1.bn.running_var', 'model.encoder.model.s2.b1.conv1.bn.num_batches_tracked', 'model.encoder.model.s2.b1.conv2.conv.weight', 'model.encoder.model.s2.b1.conv2.bn.weight', 'model.encoder.model.s2.b1.conv2.bn.bias', 'model.encoder.model.s2.b1.conv2.bn.running_mean', 'model.encoder.model.s2.b1.conv2.bn.running_var', 'model.encoder.model.s2.b1.conv2.bn.num_batches_tracked', 'model.encoder.model.s2.b1.se.fc1.weight', 'model.encoder.model.s2.b1.se.fc1.bias', 'model.encoder.model.s2.b1.se.fc2.weight', 'model.encoder.model.s2.b1.se.fc2.bias', 'model.encoder.model.s2.b1.conv3.conv.weight', 'model.encoder.model.s2.b1.conv3.bn.weight', 'model.encoder.model.s2.b1.conv3.bn.bias', 'model.encoder.model.s2.b1.conv3.bn.running_mean', 'model.encoder.model.s2.b1.conv3.bn.running_var', 'model.encoder.model.s2.b1.conv3.bn.num_batches_tracked', 'model.encoder.model.s2.b1.downsample.conv.weight', 'model.encoder.model.s2.b1.downsample.bn.weight', 'model.encoder.model.s2.b1.downsample.bn.bias', 'model.encoder.model.s2.b1.downsample.bn.running_mean', 'model.encoder.model.s2.b1.downsample.bn.running_var', 'model.encoder.model.s2.b1.downsample.bn.num_batches_tracked', 'model.encoder.model.s2.b2.conv1.conv.weight', 'model.encoder.model.s2.b2.conv1.bn.weight', 'model.encoder.model.s2.b2.conv1.bn.bias', 'model.encoder.model.s2.b2.conv1.bn.running_mean', 'model.encoder.model.s2.b2.conv1.bn.running_var', 'model.encoder.model.s2.b2.conv1.bn.num_batches_tracked', 'model.encoder.model.s2.b2.conv2.conv.weight', 'model.encoder.model.s2.b2.conv2.bn.weight', 'model.encoder.model.s2.b2.conv2.bn.bias', 'model.encoder.model.s2.b2.conv2.bn.running_mean', 'model.encoder.model.s2.b2.conv2.bn.running_var', 'model.encoder.model.s2.b2.conv2.bn.num_batches_tracked', 'model.encoder.model.s2.b2.se.fc1.weight', 'model.encoder.model.s2.b2.se.fc1.bias', 'model.encoder.model.s2.b2.se.fc2.weight', 'model.encoder.model.s2.b2.se.fc2.bias', 'model.encoder.model.s2.b2.conv3.conv.weight', 'model.encoder.model.s2.b2.conv3.bn.weight', 'model.encoder.model.s2.b2.conv3.bn.bias', 'model.encoder.model.s2.b2.conv3.bn.running_mean', 'model.encoder.model.s2.b2.conv3.bn.running_var', 'model.encoder.model.s2.b2.conv3.bn.num_batches_tracked', 'model.encoder.model.s2.b3.conv1.conv.weight', 'model.encoder.model.s2.b3.conv1.bn.weight', 'model.encoder.model.s2.b3.conv1.bn.bias', 'model.encoder.model.s2.b3.conv1.bn.running_mean', 'model.encoder.model.s2.b3.conv1.bn.running_var', 'model.encoder.model.s2.b3.conv1.bn.num_batches_tracked', 'model.encoder.model.s2.b3.conv2.conv.weight', 'model.encoder.model.s2.b3.conv2.bn.weight', 'model.encoder.model.s2.b3.conv2.bn.bias', 'model.encoder.model.s2.b3.conv2.bn.running_mean', 'model.encoder.model.s2.b3.conv2.bn.running_var', 'model.encoder.model.s2.b3.conv2.bn.num_batches_tracked', 'model.encoder.model.s2.b3.se.fc1.weight', 'model.encoder.model.s2.b3.se.fc1.bias', 'model.encoder.model.s2.b3.se.fc2.weight', 'model.encoder.model.s2.b3.se.fc2.bias', 'model.encoder.model.s2.b3.conv3.conv.weight', 'model.encoder.model.s2.b3.conv3.bn.weight', 'model.encoder.model.s2.b3.conv3.bn.bias', 'model.encoder.model.s2.b3.conv3.bn.running_mean', 'model.encoder.model.s2.b3.conv3.bn.running_var', 'model.encoder.model.s2.b3.conv3.bn.num_batches_tracked', 'model.encoder.model.s2.b4.conv1.conv.weight', 'model.encoder.model.s2.b4.conv1.bn.weight', 'model.encoder.model.s2.b4.conv1.bn.bias', 'model.encoder.model.s2.b4.conv1.bn.running_mean', 'model.encoder.model.s2.b4.conv1.bn.running_var', 'model.encoder.model.s2.b4.conv1.bn.num_batches_tracked', 'model.encoder.model.s2.b4.conv2.conv.weight', 'model.encoder.model.s2.b4.conv2.bn.weight', 'model.encoder.model.s2.b4.conv2.bn.bias', 'model.encoder.model.s2.b4.conv2.bn.running_mean', 'model.encoder.model.s2.b4.conv2.bn.running_var', 'model.encoder.model.s2.b4.conv2.bn.num_batches_tracked', 'model.encoder.model.s2.b4.se.fc1.weight', 'model.encoder.model.s2.b4.se.fc1.bias', 'model.encoder.model.s2.b4.se.fc2.weight', 'model.encoder.model.s2.b4.se.fc2.bias', 'model.encoder.model.s2.b4.conv3.conv.weight', 'model.encoder.model.s2.b4.conv3.bn.weight', 'model.encoder.model.s2.b4.conv3.bn.bias', 'model.encoder.model.s2.b4.conv3.bn.running_mean', 'model.encoder.model.s2.b4.conv3.bn.running_var', 'model.encoder.model.s2.b4.conv3.bn.num_batches_tracked', 'model.encoder.model.s2.b5.conv1.conv.weight', 'model.encoder.model.s2.b5.conv1.bn.weight', 'model.encoder.model.s2.b5.conv1.bn.bias', 'model.encoder.model.s2.b5.conv1.bn.running_mean', 'model.encoder.model.s2.b5.conv1.bn.running_var', 'model.encoder.model.s2.b5.conv1.bn.num_batches_tracked', 'model.encoder.model.s2.b5.conv2.conv.weight', 'model.encoder.model.s2.b5.conv2.bn.weight', 'model.encoder.model.s2.b5.conv2.bn.bias', 'model.encoder.model.s2.b5.conv2.bn.running_mean', 'model.encoder.model.s2.b5.conv2.bn.running_var', 'model.encoder.model.s2.b5.conv2.bn.num_batches_tracked', 'model.encoder.model.s2.b5.se.fc1.weight', 'model.encoder.model.s2.b5.se.fc1.bias', 'model.encoder.model.s2.b5.se.fc2.weight', 'model.encoder.model.s2.b5.se.fc2.bias', 'model.encoder.model.s2.b5.conv3.conv.weight', 'model.encoder.model.s2.b5.conv3.bn.weight', 'model.encoder.model.s2.b5.conv3.bn.bias', 'model.encoder.model.s2.b5.conv3.bn.running_mean', 'model.encoder.model.s2.b5.conv3.bn.running_var', 'model.encoder.model.s2.b5.conv3.bn.num_batches_tracked', 'model.encoder.model.s3.b1.conv1.conv.weight', 'model.encoder.model.s3.b1.conv1.bn.weight', 'model.encoder.model.s3.b1.conv1.bn.bias', 'model.encoder.model.s3.b1.conv1.bn.running_mean', 'model.encoder.model.s3.b1.conv1.bn.running_var', 'model.encoder.model.s3.b1.conv1.bn.num_batches_tracked', 'model.encoder.model.s3.b1.conv2.conv.weight', 'model.encoder.model.s3.b1.conv2.bn.weight', 'model.encoder.model.s3.b1.conv2.bn.bias', 'model.encoder.model.s3.b1.conv2.bn.running_mean', 'model.encoder.model.s3.b1.conv2.bn.running_var', 'model.encoder.model.s3.b1.conv2.bn.num_batches_tracked', 'model.encoder.model.s3.b1.se.fc1.weight', 'model.encoder.model.s3.b1.se.fc1.bias', 'model.encoder.model.s3.b1.se.fc2.weight', 'model.encoder.model.s3.b1.se.fc2.bias', 'model.encoder.model.s3.b1.conv3.conv.weight', 'model.encoder.model.s3.b1.conv3.bn.weight', 'model.encoder.model.s3.b1.conv3.bn.bias', 'model.encoder.model.s3.b1.conv3.bn.running_mean', 'model.encoder.model.s3.b1.conv3.bn.running_var', 'model.encoder.model.s3.b1.conv3.bn.num_batches_tracked', 'model.encoder.model.s3.b1.downsample.conv.weight', 'model.encoder.model.s3.b1.downsample.bn.weight', 'model.encoder.model.s3.b1.downsample.bn.bias', 'model.encoder.model.s3.b1.downsample.bn.running_mean', 'model.encoder.model.s3.b1.downsample.bn.running_var', 'model.encoder.model.s3.b1.downsample.bn.num_batches_tracked', 'model.encoder.model.s3.b2.conv1.conv.weight', 'model.encoder.model.s3.b2.conv1.bn.weight', 'model.encoder.model.s3.b2.conv1.bn.bias', 'model.encoder.model.s3.b2.conv1.bn.running_mean', 'model.encoder.model.s3.b2.conv1.bn.running_var', 'model.encoder.model.s3.b2.conv1.bn.num_batches_tracked', 'model.encoder.model.s3.b2.conv2.conv.weight', 'model.encoder.model.s3.b2.conv2.bn.weight', 'model.encoder.model.s3.b2.conv2.bn.bias', 'model.encoder.model.s3.b2.conv2.bn.running_mean', 'model.encoder.model.s3.b2.conv2.bn.running_var', 'model.encoder.model.s3.b2.conv2.bn.num_batches_tracked', 'model.encoder.model.s3.b2.se.fc1.weight', 'model.encoder.model.s3.b2.se.fc1.bias', 'model.encoder.model.s3.b2.se.fc2.weight', 'model.encoder.model.s3.b2.se.fc2.bias', 'model.encoder.model.s3.b2.conv3.conv.weight', 'model.encoder.model.s3.b2.conv3.bn.weight', 'model.encoder.model.s3.b2.conv3.bn.bias', 'model.encoder.model.s3.b2.conv3.bn.running_mean', 'model.encoder.model.s3.b2.conv3.bn.running_var', 'model.encoder.model.s3.b2.conv3.bn.num_batches_tracked', 'model.encoder.model.s3.b3.conv1.conv.weight', 'model.encoder.model.s3.b3.conv1.bn.weight', 'model.encoder.model.s3.b3.conv1.bn.bias', 'model.encoder.model.s3.b3.conv1.bn.running_mean', 'model.encoder.model.s3.b3.conv1.bn.running_var', 'model.encoder.model.s3.b3.conv1.bn.num_batches_tracked', 'model.encoder.model.s3.b3.conv2.conv.weight', 'model.encoder.model.s3.b3.conv2.bn.weight', 'model.encoder.model.s3.b3.conv2.bn.bias', 'model.encoder.model.s3.b3.conv2.bn.running_mean', 'model.encoder.model.s3.b3.conv2.bn.running_var', 'model.encoder.model.s3.b3.conv2.bn.num_batches_tracked', 'model.encoder.model.s3.b3.se.fc1.weight', 'model.encoder.model.s3.b3.se.fc1.bias', 'model.encoder.model.s3.b3.se.fc2.weight', 'model.encoder.model.s3.b3.se.fc2.bias', 'model.encoder.model.s3.b3.conv3.conv.weight', 'model.encoder.model.s3.b3.conv3.bn.weight', 'model.encoder.model.s3.b3.conv3.bn.bias', 'model.encoder.model.s3.b3.conv3.bn.running_mean', 'model.encoder.model.s3.b3.conv3.bn.running_var', 'model.encoder.model.s3.b3.conv3.bn.num_batches_tracked', 'model.encoder.model.s3.b4.conv1.conv.weight', 'model.encoder.model.s3.b4.conv1.bn.weight', 'model.encoder.model.s3.b4.conv1.bn.bias', 'model.encoder.model.s3.b4.conv1.bn.running_mean', 'model.encoder.model.s3.b4.conv1.bn.running_var', 'model.encoder.model.s3.b4.conv1.bn.num_batches_tracked', 'model.encoder.model.s3.b4.conv2.conv.weight', 'model.encoder.model.s3.b4.conv2.bn.weight', 'model.encoder.model.s3.b4.conv2.bn.bias', 'model.encoder.model.s3.b4.conv2.bn.running_mean', 'model.encoder.model.s3.b4.conv2.bn.running_var', 'model.encoder.model.s3.b4.conv2.bn.num_batches_tracked', 'model.encoder.model.s3.b4.se.fc1.weight', 'model.encoder.model.s3.b4.se.fc1.bias', 'model.encoder.model.s3.b4.se.fc2.weight', 'model.encoder.model.s3.b4.se.fc2.bias', 'model.encoder.model.s3.b4.conv3.conv.weight', 'model.encoder.model.s3.b4.conv3.bn.weight', 'model.encoder.model.s3.b4.conv3.bn.bias', 'model.encoder.model.s3.b4.conv3.bn.running_mean', 'model.encoder.model.s3.b4.conv3.bn.running_var', 'model.encoder.model.s3.b4.conv3.bn.num_batches_tracked', 'model.encoder.model.s3.b5.conv1.conv.weight', 'model.encoder.model.s3.b5.conv1.bn.weight', 'model.encoder.model.s3.b5.conv1.bn.bias', 'model.encoder.model.s3.b5.conv1.bn.running_mean', 'model.encoder.model.s3.b5.conv1.bn.running_var', 'model.encoder.model.s3.b5.conv1.bn.num_batches_tracked', 'model.encoder.model.s3.b5.conv2.conv.weight', 'model.encoder.model.s3.b5.conv2.bn.weight', 'model.encoder.model.s3.b5.conv2.bn.bias', 'model.encoder.model.s3.b5.conv2.bn.running_mean', 'model.encoder.model.s3.b5.conv2.bn.running_var', 'model.encoder.model.s3.b5.conv2.bn.num_batches_tracked', 'model.encoder.model.s3.b5.se.fc1.weight', 'model.encoder.model.s3.b5.se.fc1.bias', 'model.encoder.model.s3.b5.se.fc2.weight', 'model.encoder.model.s3.b5.se.fc2.bias', 'model.encoder.model.s3.b5.conv3.conv.weight', 'model.encoder.model.s3.b5.conv3.bn.weight', 'model.encoder.model.s3.b5.conv3.bn.bias', 'model.encoder.model.s3.b5.conv3.bn.running_mean', 'model.encoder.model.s3.b5.conv3.bn.running_var', 'model.encoder.model.s3.b5.conv3.bn.num_batches_tracked', 'model.encoder.model.s3.b6.conv1.conv.weight', 'model.encoder.model.s3.b6.conv1.bn.weight', 'model.encoder.model.s3.b6.conv1.bn.bias', 'model.encoder.model.s3.b6.conv1.bn.running_mean', 'model.encoder.model.s3.b6.conv1.bn.running_var', 'model.encoder.model.s3.b6.conv1.bn.num_batches_tracked', 'model.encoder.model.s3.b6.conv2.conv.weight', 'model.encoder.model.s3.b6.conv2.bn.weight', 'model.encoder.model.s3.b6.conv2.bn.bias', 'model.encoder.model.s3.b6.conv2.bn.running_mean', 'model.encoder.model.s3.b6.conv2.bn.running_var', 'model.encoder.model.s3.b6.conv2.bn.num_batches_tracked', 'model.encoder.model.s3.b6.se.fc1.weight', 'model.encoder.model.s3.b6.se.fc1.bias', 'model.encoder.model.s3.b6.se.fc2.weight', 'model.encoder.model.s3.b6.se.fc2.bias', 'model.encoder.model.s3.b6.conv3.conv.weight', 'model.encoder.model.s3.b6.conv3.bn.weight', 'model.encoder.model.s3.b6.conv3.bn.bias', 'model.encoder.model.s3.b6.conv3.bn.running_mean', 'model.encoder.model.s3.b6.conv3.bn.running_var', 'model.encoder.model.s3.b6.conv3.bn.num_batches_tracked', 'model.encoder.model.s3.b7.conv1.conv.weight', 'model.encoder.model.s3.b7.conv1.bn.weight', 'model.encoder.model.s3.b7.conv1.bn.bias', 'model.encoder.model.s3.b7.conv1.bn.running_mean', 'model.encoder.model.s3.b7.conv1.bn.running_var', 'model.encoder.model.s3.b7.conv1.bn.num_batches_tracked', 'model.encoder.model.s3.b7.conv2.conv.weight', 'model.encoder.model.s3.b7.conv2.bn.weight', 'model.encoder.model.s3.b7.conv2.bn.bias', 'model.encoder.model.s3.b7.conv2.bn.running_mean', 'model.encoder.model.s3.b7.conv2.bn.running_var', 'model.encoder.model.s3.b7.conv2.bn.num_batches_tracked', 'model.encoder.model.s3.b7.se.fc1.weight', 'model.encoder.model.s3.b7.se.fc1.bias', 'model.encoder.model.s3.b7.se.fc2.weight', 'model.encoder.model.s3.b7.se.fc2.bias', 'model.encoder.model.s3.b7.conv3.conv.weight', 'model.encoder.model.s3.b7.conv3.bn.weight', 'model.encoder.model.s3.b7.conv3.bn.bias', 'model.encoder.model.s3.b7.conv3.bn.running_mean', 'model.encoder.model.s3.b7.conv3.bn.running_var', 'model.encoder.model.s3.b7.conv3.bn.num_batches_tracked', 'model.encoder.model.s3.b8.conv1.conv.weight', 'model.encoder.model.s3.b8.conv1.bn.weight', 'model.encoder.model.s3.b8.conv1.bn.bias', 'model.encoder.model.s3.b8.conv1.bn.running_mean', 'model.encoder.model.s3.b8.conv1.bn.running_var', 'model.encoder.model.s3.b8.conv1.bn.num_batches_tracked', 'model.encoder.model.s3.b8.conv2.conv.weight', 'model.encoder.model.s3.b8.conv2.bn.weight', 'model.encoder.model.s3.b8.conv2.bn.bias', 'model.encoder.model.s3.b8.conv2.bn.running_mean', 'model.encoder.model.s3.b8.conv2.bn.running_var', 'model.encoder.model.s3.b8.conv2.bn.num_batches_tracked', 'model.encoder.model.s3.b8.se.fc1.weight', 'model.encoder.model.s3.b8.se.fc1.bias', 'model.encoder.model.s3.b8.se.fc2.weight', 'model.encoder.model.s3.b8.se.fc2.bias', 'model.encoder.model.s3.b8.conv3.conv.weight', 'model.encoder.model.s3.b8.conv3.bn.weight', 'model.encoder.model.s3.b8.conv3.bn.bias', 'model.encoder.model.s3.b8.conv3.bn.running_mean', 'model.encoder.model.s3.b8.conv3.bn.running_var', 'model.encoder.model.s3.b8.conv3.bn.num_batches_tracked', 'model.encoder.model.s3.b9.conv1.conv.weight', 'model.encoder.model.s3.b9.conv1.bn.weight', 'model.encoder.model.s3.b9.conv1.bn.bias', 'model.encoder.model.s3.b9.conv1.bn.running_mean', 'model.encoder.model.s3.b9.conv1.bn.running_var', 'model.encoder.model.s3.b9.conv1.bn.num_batches_tracked', 'model.encoder.model.s3.b9.conv2.conv.weight', 'model.encoder.model.s3.b9.conv2.bn.weight', 'model.encoder.model.s3.b9.conv2.bn.bias', 'model.encoder.model.s3.b9.conv2.bn.running_mean', 'model.encoder.model.s3.b9.conv2.bn.running_var', 'model.encoder.model.s3.b9.conv2.bn.num_batches_tracked', 'model.encoder.model.s3.b9.se.fc1.weight', 'model.encoder.model.s3.b9.se.fc1.bias', 'model.encoder.model.s3.b9.se.fc2.weight', 'model.encoder.model.s3.b9.se.fc2.bias', 'model.encoder.model.s3.b9.conv3.conv.weight', 'model.encoder.model.s3.b9.conv3.bn.weight', 'model.encoder.model.s3.b9.conv3.bn.bias', 'model.encoder.model.s3.b9.conv3.bn.running_mean', 'model.encoder.model.s3.b9.conv3.bn.running_var', 'model.encoder.model.s3.b9.conv3.bn.num_batches_tracked', 'model.encoder.model.s3.b10.conv1.conv.weight', 'model.encoder.model.s3.b10.conv1.bn.weight', 'model.encoder.model.s3.b10.conv1.bn.bias', 'model.encoder.model.s3.b10.conv1.bn.running_mean', 'model.encoder.model.s3.b10.conv1.bn.running_var', 'model.encoder.model.s3.b10.conv1.bn.num_batches_tracked', 'model.encoder.model.s3.b10.conv2.conv.weight', 'model.encoder.model.s3.b10.conv2.bn.weight', 'model.encoder.model.s3.b10.conv2.bn.bias', 'model.encoder.model.s3.b10.conv2.bn.running_mean', 'model.encoder.model.s3.b10.conv2.bn.running_var', 'model.encoder.model.s3.b10.conv2.bn.num_batches_tracked', 'model.encoder.model.s3.b10.se.fc1.weight', 'model.encoder.model.s3.b10.se.fc1.bias', 'model.encoder.model.s3.b10.se.fc2.weight', 'model.encoder.model.s3.b10.se.fc2.bias', 'model.encoder.model.s3.b10.conv3.conv.weight', 'model.encoder.model.s3.b10.conv3.bn.weight', 'model.encoder.model.s3.b10.conv3.bn.bias', 'model.encoder.model.s3.b10.conv3.bn.running_mean', 'model.encoder.model.s3.b10.conv3.bn.running_var', 'model.encoder.model.s3.b10.conv3.bn.num_batches_tracked', 'model.encoder.model.s3.b11.conv1.conv.weight', 'model.encoder.model.s3.b11.conv1.bn.weight', 'model.encoder.model.s3.b11.conv1.bn.bias', 'model.encoder.model.s3.b11.conv1.bn.running_mean', 'model.encoder.model.s3.b11.conv1.bn.running_var', 'model.encoder.model.s3.b11.conv1.bn.num_batches_tracked', 'model.encoder.model.s3.b11.conv2.conv.weight', 'model.encoder.model.s3.b11.conv2.bn.weight', 'model.encoder.model.s3.b11.conv2.bn.bias', 'model.encoder.model.s3.b11.conv2.bn.running_mean', 'model.encoder.model.s3.b11.conv2.bn.running_var', 'model.encoder.model.s3.b11.conv2.bn.num_batches_tracked', 'model.encoder.model.s3.b11.se.fc1.weight', 'model.encoder.model.s3.b11.se.fc1.bias', 'model.encoder.model.s3.b11.se.fc2.weight', 'model.encoder.model.s3.b11.se.fc2.bias', 'model.encoder.model.s3.b11.conv3.conv.weight', 'model.encoder.model.s3.b11.conv3.bn.weight', 'model.encoder.model.s3.b11.conv3.bn.bias', 'model.encoder.model.s3.b11.conv3.bn.running_mean', 'model.encoder.model.s3.b11.conv3.bn.running_var', 'model.encoder.model.s3.b11.conv3.bn.num_batches_tracked', 'model.encoder.model.s4.b1.conv1.conv.weight', 'model.encoder.model.s4.b1.conv1.bn.weight', 'model.encoder.model.s4.b1.conv1.bn.bias', 'model.encoder.model.s4.b1.conv1.bn.running_mean', 'model.encoder.model.s4.b1.conv1.bn.running_var', 'model.encoder.model.s4.b1.conv1.bn.num_batches_tracked', 'model.encoder.model.s4.b1.conv2.conv.weight', 'model.encoder.model.s4.b1.conv2.bn.weight', 'model.encoder.model.s4.b1.conv2.bn.bias', 'model.encoder.model.s4.b1.conv2.bn.running_mean', 'model.encoder.model.s4.b1.conv2.bn.running_var', 'model.encoder.model.s4.b1.conv2.bn.num_batches_tracked', 'model.encoder.model.s4.b1.se.fc1.weight', 'model.encoder.model.s4.b1.se.fc1.bias', 'model.encoder.model.s4.b1.se.fc2.weight', 'model.encoder.model.s4.b1.se.fc2.bias', 'model.encoder.model.s4.b1.conv3.conv.weight', 'model.encoder.model.s4.b1.conv3.bn.weight', 'model.encoder.model.s4.b1.conv3.bn.bias', 'model.encoder.model.s4.b1.conv3.bn.running_mean', 'model.encoder.model.s4.b1.conv3.bn.running_var', 'model.encoder.model.s4.b1.conv3.bn.num_batches_tracked', 'model.encoder.model.s4.b1.downsample.conv.weight', 'model.encoder.model.s4.b1.downsample.bn.weight', 'model.encoder.model.s4.b1.downsample.bn.bias', 'model.encoder.model.s4.b1.downsample.bn.running_mean', 'model.encoder.model.s4.b1.downsample.bn.running_var', 'model.encoder.model.s4.b1.downsample.bn.num_batches_tracked', 'model.decoder.blocks.x_0_0.conv1.0.weight', 'model.decoder.blocks.x_0_0.conv1.1.weight', 'model.decoder.blocks.x_0_0.conv1.1.bias', 'model.decoder.blocks.x_0_0.conv1.1.running_mean', 'model.decoder.blocks.x_0_0.conv1.1.running_var', 'model.decoder.blocks.x_0_0.conv1.1.num_batches_tracked', 'model.decoder.blocks.x_0_0.conv2.0.weight', 'model.decoder.blocks.x_0_0.conv2.1.weight', 'model.decoder.blocks.x_0_0.conv2.1.bias', 'model.decoder.blocks.x_0_0.conv2.1.running_mean', 'model.decoder.blocks.x_0_0.conv2.1.running_var', 'model.decoder.blocks.x_0_0.conv2.1.num_batches_tracked', 'model.decoder.blocks.x_0_1.conv1.0.weight', 'model.decoder.blocks.x_0_1.conv1.1.weight', 'model.decoder.blocks.x_0_1.conv1.1.bias', 'model.decoder.blocks.x_0_1.conv1.1.running_mean', 'model.decoder.blocks.x_0_1.conv1.1.running_var', 'model.decoder.blocks.x_0_1.conv1.1.num_batches_tracked', 'model.decoder.blocks.x_0_1.conv2.0.weight', 'model.decoder.blocks.x_0_1.conv2.1.weight', 'model.decoder.blocks.x_0_1.conv2.1.bias', 'model.decoder.blocks.x_0_1.conv2.1.running_mean', 'model.decoder.blocks.x_0_1.conv2.1.running_var', 'model.decoder.blocks.x_0_1.conv2.1.num_batches_tracked', 'model.decoder.blocks.x_1_1.conv1.0.weight', 'model.decoder.blocks.x_1_1.conv1.1.weight', 'model.decoder.blocks.x_1_1.conv1.1.bias', 'model.decoder.blocks.x_1_1.conv1.1.running_mean', 'model.decoder.blocks.x_1_1.conv1.1.running_var', 'model.decoder.blocks.x_1_1.conv1.1.num_batches_tracked', 'model.decoder.blocks.x_1_1.conv2.0.weight', 'model.decoder.blocks.x_1_1.conv2.1.weight', 'model.decoder.blocks.x_1_1.conv2.1.bias', 'model.decoder.blocks.x_1_1.conv2.1.running_mean', 'model.decoder.blocks.x_1_1.conv2.1.running_var', 'model.decoder.blocks.x_1_1.conv2.1.num_batches_tracked', 'model.decoder.blocks.x_0_2.conv1.0.weight', 'model.decoder.blocks.x_0_2.conv1.1.weight', 'model.decoder.blocks.x_0_2.conv1.1.bias', 'model.decoder.blocks.x_0_2.conv1.1.running_mean', 'model.decoder.blocks.x_0_2.conv1.1.running_var', 'model.decoder.blocks.x_0_2.conv1.1.num_batches_tracked', 'model.decoder.blocks.x_0_2.conv2.0.weight', 'model.decoder.blocks.x_0_2.conv2.1.weight', 'model.decoder.blocks.x_0_2.conv2.1.bias', 'model.decoder.blocks.x_0_2.conv2.1.running_mean', 'model.decoder.blocks.x_0_2.conv2.1.running_var', 'model.decoder.blocks.x_0_2.conv2.1.num_batches_tracked', 'model.decoder.blocks.x_1_2.conv1.0.weight', 'model.decoder.blocks.x_1_2.conv1.1.weight', 'model.decoder.blocks.x_1_2.conv1.1.bias', 'model.decoder.blocks.x_1_2.conv1.1.running_mean', 'model.decoder.blocks.x_1_2.conv1.1.running_var', 'model.decoder.blocks.x_1_2.conv1.1.num_batches_tracked', 'model.decoder.blocks.x_1_2.conv2.0.weight', 'model.decoder.blocks.x_1_2.conv2.1.weight', 'model.decoder.blocks.x_1_2.conv2.1.bias', 'model.decoder.blocks.x_1_2.conv2.1.running_mean', 'model.decoder.blocks.x_1_2.conv2.1.running_var', 'model.decoder.blocks.x_1_2.conv2.1.num_batches_tracked', 'model.decoder.blocks.x_2_2.conv1.0.weight', 'model.decoder.blocks.x_2_2.conv1.1.weight', 'model.decoder.blocks.x_2_2.conv1.1.bias', 'model.decoder.blocks.x_2_2.conv1.1.running_mean', 'model.decoder.blocks.x_2_2.conv1.1.running_var', 'model.decoder.blocks.x_2_2.conv1.1.num_batches_tracked', 'model.decoder.blocks.x_2_2.conv2.0.weight', 'model.decoder.blocks.x_2_2.conv2.1.weight', 'model.decoder.blocks.x_2_2.conv2.1.bias', 'model.decoder.blocks.x_2_2.conv2.1.running_mean', 'model.decoder.blocks.x_2_2.conv2.1.running_var', 'model.decoder.blocks.x_2_2.conv2.1.num_batches_tracked', 'model.decoder.blocks.x_0_3.conv1.0.weight', 'model.decoder.blocks.x_0_3.conv1.1.weight', 'model.decoder.blocks.x_0_3.conv1.1.bias', 'model.decoder.blocks.x_0_3.conv1.1.running_mean', 'model.decoder.blocks.x_0_3.conv1.1.running_var', 'model.decoder.blocks.x_0_3.conv1.1.num_batches_tracked', 'model.decoder.blocks.x_0_3.conv2.0.weight', 'model.decoder.blocks.x_0_3.conv2.1.weight', 'model.decoder.blocks.x_0_3.conv2.1.bias', 'model.decoder.blocks.x_0_3.conv2.1.running_mean', 'model.decoder.blocks.x_0_3.conv2.1.running_var', 'model.decoder.blocks.x_0_3.conv2.1.num_batches_tracked', 'model.decoder.blocks.x_1_3.conv1.0.weight', 'model.decoder.blocks.x_1_3.conv1.1.weight', 'model.decoder.blocks.x_1_3.conv1.1.bias', 'model.decoder.blocks.x_1_3.conv1.1.running_mean', 'model.decoder.blocks.x_1_3.conv1.1.running_var', 'model.decoder.blocks.x_1_3.conv1.1.num_batches_tracked', 'model.decoder.blocks.x_1_3.conv2.0.weight', 'model.decoder.blocks.x_1_3.conv2.1.weight', 'model.decoder.blocks.x_1_3.conv2.1.bias', 'model.decoder.blocks.x_1_3.conv2.1.running_mean', 'model.decoder.blocks.x_1_3.conv2.1.running_var', 'model.decoder.blocks.x_1_3.conv2.1.num_batches_tracked', 'model.decoder.blocks.x_2_3.conv1.0.weight', 'model.decoder.blocks.x_2_3.conv1.1.weight', 'model.decoder.blocks.x_2_3.conv1.1.bias', 'model.decoder.blocks.x_2_3.conv1.1.running_mean', 'model.decoder.blocks.x_2_3.conv1.1.running_var', 'model.decoder.blocks.x_2_3.conv1.1.num_batches_tracked', 'model.decoder.blocks.x_2_3.conv2.0.weight', 'model.decoder.blocks.x_2_3.conv2.1.weight', 'model.decoder.blocks.x_2_3.conv2.1.bias', 'model.decoder.blocks.x_2_3.conv2.1.running_mean', 'model.decoder.blocks.x_2_3.conv2.1.running_var', 'model.decoder.blocks.x_2_3.conv2.1.num_batches_tracked', 'model.decoder.blocks.x_3_3.conv1.0.weight', 'model.decoder.blocks.x_3_3.conv1.1.weight', 'model.decoder.blocks.x_3_3.conv1.1.bias', 'model.decoder.blocks.x_3_3.conv1.1.running_mean', 'model.decoder.blocks.x_3_3.conv1.1.running_var', 'model.decoder.blocks.x_3_3.conv1.1.num_batches_tracked', 'model.decoder.blocks.x_3_3.conv2.0.weight', 'model.decoder.blocks.x_3_3.conv2.1.weight', 'model.decoder.blocks.x_3_3.conv2.1.bias', 'model.decoder.blocks.x_3_3.conv2.1.running_mean', 'model.decoder.blocks.x_3_3.conv2.1.running_var', 'model.decoder.blocks.x_3_3.conv2.1.num_batches_tracked', 'model.decoder.blocks.x_0_4.conv1.0.weight', 'model.decoder.blocks.x_0_4.conv1.1.weight', 'model.decoder.blocks.x_0_4.conv1.1.bias', 'model.decoder.blocks.x_0_4.conv1.1.running_mean', 'model.decoder.blocks.x_0_4.conv1.1.running_var', 'model.decoder.blocks.x_0_4.conv1.1.num_batches_tracked', 'model.decoder.blocks.x_0_4.conv2.0.weight', 'model.decoder.blocks.x_0_4.conv2.1.weight', 'model.decoder.blocks.x_0_4.conv2.1.bias', 'model.decoder.blocks.x_0_4.conv2.1.running_mean', 'model.decoder.blocks.x_0_4.conv2.1.running_var', 'model.decoder.blocks.x_0_4.conv2.1.num_batches_tracked', 'model.segmentation_head.0.weight', 'model.segmentation_head.0.bias'])\n"
     ]
    }
   ],
   "source": [
    "#coregir dict\n",
    "\n",
    "old_state_dict = torch.load('inference_model.pth')\n",
    "new_state_dict = {}\n",
    "\n",
    "for key, value in old_state_dict.items():\n",
    "    new_key = key.replace(\"model.encoder.\", \"model.encoder.model.\")\n",
    "    new_state_dict[new_key] = value\n",
    "\n",
    "print(new_state_dict.keys())\n",
    "torch.save(new_state_dict, \"new_inference_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de7ba004-8536-4c08-8e21-6d1c3fd474fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "# model\n",
    "device = select_device()\n",
    "model=Unet()\n",
    "model.load_state_dict(torch.load('new_inference_model.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278f4376-4241-40b8-a935-dab7a0f34d9c",
   "metadata": {},
   "source": [
    "# Create Directory for Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af4fcf66-5369-4f97-967d-0131103c87e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath='./output/'\n",
    "if not os.path.exists(outputPath):\n",
    "    os.makedirs(outputPath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87cb843-0f6f-4d75-ab06-7be1617c78bd",
   "metadata": {},
   "source": [
    "# Set up data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7c9d3c4-037d-4916-bb75-94fba67506bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data =\"./Modelling_Dataset/png/\"\n",
    "\n",
    "files=os.listdir(path_to_data)\n",
    "files=[x for x in files if 'label' not in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ebdf84-cfce-4a9a-b2bd-9e45614d8b3a",
   "metadata": {},
   "source": [
    "# Load measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76a9de9d-5f40-4801-84b5-b20b733a273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these measures not only include the used measures mentioned in the publication, but also additional measures used for further testing\n",
    "# the following measures contain the 90 quantile as well as the median of the nuclei areas in the whole training data as well as the solidity measures of different quantiles\n",
    "trainNineQuantile = np.load('./measures/90_quantile_allTrain.npy')\n",
    "trainMedian = np.load('./measures/median_allTrain.npy')*2\n",
    "\n",
    "sol_ninety = np.load('./measures/sol_ninety.npy')\n",
    "sol_ninetyfive = np.load('./measures/sol_ninetyfive.npy')\n",
    "sol_ninetyeight = np.load('./measures/sol_ninetyeight.npy')\n",
    "sol_ten = np.load('./measures/sol_ten.npy')\n",
    "sol_five = np.load('./measures/sol_five.npy')\n",
    "sol_two = np.load('./measures/sol_two.npy')\n",
    "\n",
    "sol_highQuantiles=[sol_ninety,sol_ninetyfive,sol_ninetyeight]\n",
    "sol_lowQuantiles=[sol_ten,sol_five,sol_two]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0209db2-98f0-463e-a5d8-6261fec288eb",
   "metadata": {},
   "source": [
    "# Create filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2b594b8-baab-49e7-a3f7-77a6aeac7da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "multipleFilesPerCase=False # required if there a more than one slide per file -> separator has to be set correctly depending on the file naming\n",
    "sep='_'\n",
    "if(multipleFilesPerCase):\n",
    "    filesById=[x.split(sep)[0] for x in files]\n",
    "else:\n",
    "    filesById=files\n",
    "\n",
    "filesById=np.unique(filesById)\n",
    "fileMap={}\n",
    "for file in filesById:\n",
    "    fileMap[file]=[x for x in files if file in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43d3543-0ba1-4010-91c4-3f1a06085ee5",
   "metadata": {},
   "source": [
    "# Apply Model and create measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85eb5b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_and_conquer(img, divs):\n",
    "    h, w, _ = img.shape\n",
    "    divh = h//divs[0]\n",
    "    divw = w//divs[0]\n",
    "    output = np.zeros((h, w))\n",
    "    for i in range(divs[0]):\n",
    "        for j in range(divs[1]):\n",
    "            imgk = img[divh*i:divh*(i+1), divw*j:divw*(j+1)]\n",
    "            outputk = applyModeltoGetIdMask(model,imgk,resShape=-1,do_filter=True)\n",
    "            output[divh*i:divh*(i+1), divw*j:divw*(j+1)] = outputk\n",
    "    \n",
    "    output[output>0] = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae129359-1f2e-4685-93c3-455c59c29d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5/85 [00:48<12:57,  9.71s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m imgRaw = imread(path_to_data+crop)[:\u001b[32m1200\u001b[39m,:\u001b[32m1600\u001b[39m,:\u001b[32m3\u001b[39m]\n\u001b[32m     12\u001b[39m img=imgRaw.astype(\u001b[33m'\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m'\u001b[39m)/\u001b[32m255.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m output = \u001b[43mdivide_and_conquer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#measure image\u001b[39;00m\n\u001b[32m     17\u001b[39m cropM,a=measureImg(output,minDia=\u001b[32m3\u001b[39m,factor=\u001b[32m0.248\u001b[39m,trainNineQuantile=trainNineQuantile,trainMedian=trainMedian,sol_highQuantiles=sol_highQuantiles,sol_lowQuantiles=sol_lowQuantiles)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mdivide_and_conquer\u001b[39m\u001b[34m(img, divs)\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(divs[\u001b[32m1\u001b[39m]):\n\u001b[32m      8\u001b[39m         imgk = img[divh*i:divh*(i+\u001b[32m1\u001b[39m), divw*j:divw*(j+\u001b[32m1\u001b[39m)]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         outputk = \u001b[43mapplyModeltoGetIdMask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimgk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mresShape\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdo_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m         output[divh*i:divh*(i+\u001b[32m1\u001b[39m), divw*j:divw*(j+\u001b[32m1\u001b[39m)] = outputk\n\u001b[32m     12\u001b[39m output[output>\u001b[32m0\u001b[39m] = \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mapplyModeltoGetIdMask\u001b[39m\u001b[34m(model, img, resShape, mycMinDia, factor, do_filter)\u001b[39m\n\u001b[32m     10\u001b[39m t_img=torch.tensor(imgResh.transpose(\u001b[32m2\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m)).unsqueeze(dim=\u001b[32m0\u001b[39m)\n\u001b[32m     11\u001b[39m t_img=t_img.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m seg=\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_img\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m.squeeze().numpy()\n\u001b[32m     13\u001b[39m seg=seg[top:resShape-bottom, left:resShape-right]\n\u001b[32m     14\u001b[39m seg[seg>=\u001b[32m0.5\u001b[39m]=\u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "combinedFileMeasures={}\n",
    "fileMeasures={}\n",
    "areas={}\n",
    "nucleiCount={}\n",
    "for file in tqdm(filesById):\n",
    "    areas[file]=[]\n",
    "    cropMeasures=[]\n",
    "    cropNuclei=[]\n",
    "    for crop in fileMap[file]:\n",
    "        #load file -> has to be changed for different datatypes\n",
    "        imgRaw = imread(path_to_data+crop)[:1200,:1600,:3]\n",
    "        img=imgRaw.astype('float32')/255.0\n",
    "        \n",
    "        output = divide_and_conquer(img, (2, 2))\n",
    "\n",
    "        #measure image\n",
    "        cropM,a=measureImg(output,minDia=3,factor=0.248,trainNineQuantile=trainNineQuantile,trainMedian=trainMedian,sol_highQuantiles=sol_highQuantiles,sol_lowQuantiles=sol_lowQuantiles)\n",
    "        areas[file].extend(a)\n",
    "        cropMeasures.append(cropM)\n",
    "        fileMeasures[crop]=cropM\n",
    "        \n",
    "        cropNuclei.append(len(np.unique(output))-1)\n",
    "        \n",
    "        #recolored output -> just for visualization\n",
    "        #for idd in list(np.unique(output))[1:]:\n",
    "        #    output[np.where(output==idd)]=random.randint(127,255)\n",
    "        \n",
    "        fig, ax = plt.subplots(dpi=300)\n",
    "        ax.set_xticks([], [])\n",
    "        ax.set_yticks([], [])\n",
    "        ax.imshow(np.array(img.squeeze()))\n",
    "        ax.imshow(output, cmap='jet', alpha=0.4)\n",
    "        plt.savefig(outputPath+file+'.png', bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "        \n",
    "        \n",
    "    nucleiCount[file]=np.mean(cropNuclei)\n",
    "    combinedFileMeasures[file]=np.mean(cropMeasures,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa95d0-360b-48e3-b5c6-f478d689debb",
   "metadata": {},
   "source": [
    "# Setup columns for measured values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a6e499-70d9-452a-bd5f-4dd7786c6c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=['eccentricity','solidity','mu_area','mu_equArea']\n",
    "c=['std','mean', 'median', 'skew']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c92d4-0012-41cb-aa78-bbdd4c577d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=[]\n",
    "cols.append('file')\n",
    "for cc in c:\n",
    "    for mm in m:\n",
    "        cols.append('_'.join([cc,mm]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c6bc49-3282-4c63-9c18-b08f58ba6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.extend(['median_top_10%','mean_top_10%','90%quantile/median(areas)','90%quantile/median(log(areas))','% above 90 Quantile (Training Data Split)','% above double Median (Training Data Split)','90 Quantile','median(log(areas))','median(areas)','trainNinetyQuantile','doubleMedian','% above_solidity_ninetyQuant','% above_solidity_ninetyfiveQuant','% above_solidity_ninetyeightQuant','% below_solidity_ninetyQuant','% below_solidity_ninetyfiveQuant','% below_solidity_ninetyeightQuant','ninetyQuant','ninetyfiveQuant','ninetyeightQuant','% below_solidity_tenQuant','% below_solidity_fiveQuant','% below_solidity_twoQuant','tenQuant','fiveQuant','twoQuant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aff83f-582a-46ea-9aea-6dd886e99c40",
   "metadata": {},
   "source": [
    "# Setup dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e750f02-b5d5-4818-8140-9beafd3e4270",
   "metadata": {},
   "outputs": [],
   "source": [
    "singleMeasureDF = pd.DataFrame(columns=cols)\n",
    "i=0\n",
    "for f in fileMeasures.keys():\n",
    "    \n",
    "    entry=[f]\n",
    "    entry.extend(fileMeasures[f])\n",
    "    \n",
    "    singleMeasureDF.loc[i] = entry\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717799f5-625c-4a47-8598-f7e4775c2cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(multipleFilesPerCase):\n",
    "    combinedMeasureDF = pd.DataFrame(columns=cols)\n",
    "    i=0\n",
    "    for f in combinedFileMeasures.keys():\n",
    "\n",
    "        entry=[f]\n",
    "        entry.extend(combinedFileMeasures[f])\n",
    "\n",
    "        combinedMeasureDF.loc[i] = entry\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ffdabd-131a-4288-a03e-29f389fccbe4",
   "metadata": {},
   "source": [
    "# Save dataframe als Excel File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caacec45-d603-4ecf-a9a9-66ae1057b93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create excel output\n",
    "writer = pd.ExcelWriter(outputPath+'output.xlsx', engine='xlsxwriter')\n",
    "\n",
    "if(multipleFilesPerCase):\n",
    "    combinedMeasureDF.to_excel(writer, sheet_name='Case', index=False)\n",
    "singleMeasureDF.to_excel(writer, sheet_name='Files', index=False)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6953c674-10fc-420f-bef4-48d70a974d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
